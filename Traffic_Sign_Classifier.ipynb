{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS=20#40#20#10\n",
    "BATCH_SIZE=1280#500#128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_file = 'traffic-signs-data/train.p'\n",
    "validation_file='traffic-signs-data/valid.p'\n",
    "testing_file = 'traffic-signs-data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngll5u2MzBIYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fba3149710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow((np.subtract(X_train[80],128)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Summary of the Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 34799\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "n_train = len(y_train)\n",
    "\n",
    "n_validation = len(y_valid)\n",
    "\n",
    "n_test = len(y_test)\n",
    "\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "\n",
    "n_classes = len(set(y_train))\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_array_to_float(arr):\n",
    "    arr=np.float32(arr)\n",
    "    return arr\n",
    "def Convert_images(X_train, X_valid, X_test):\n",
    "    X_train=Convert_array_to_float(X_train)\n",
    "    X_valid=Convert_array_to_float(X_valid)\n",
    "    X_test=Convert_array_to_float(X_test)\n",
    "    return X_train, X_valid, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test=Convert_images(X_train, X_valid, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include an exploratory visualization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train,y_train=shuffle(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFsAAABZCAYAAABR/liSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYY+IrbBHxFbYI2Ir7BGxFfaI+D++IYNglUuzwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fba3894710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Data exploration v\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "index=random.randint(0,len(X_train))\n",
    "image=X_train[index].squeeze()\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image)\n",
    "print(y_train[index])\n",
    "#print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_Image_to_Array(imgarray,labels,image, image_class):\n",
    "    image2=np.expand_dims(image,axis=0) #add a 0-indexed dimension thus it is now image[0] gives actual image\n",
    "    #print(image2.shape)\n",
    "    #print(imgarray.shape)\n",
    "    imgarray=np.vstack((imgarray,image2)) #vertically stack image to image array\n",
    "    labels=np.append(labels,image_class) #add label to existing vector of labels\n",
    "    return imgarray, labels\n",
    "def Balanced_Resampling(X,y, n_classes):\n",
    "    ideal_ratio=1.0/n_classes\n",
    "    act_ratio=np.histogram(y, len(set(y)))[0]/len(X) #gets the actual proportions of the data\n",
    "    counts=np.histogram(y, len(set(y)))\n",
    "    percent_of_train_data=(counts[0]/len(X))\n",
    "    min_count=sorted(percent_of_train_data)[0]\n",
    "    #flag_alteration=[0,1,2,3,4] #0: do nothing, 1 is noisify, 2 is translate, 3 is rotate, 4 is blur\n",
    "    if min_count< ideal_ratio:\n",
    "        index_under_rep=[iter for iter, j in enumerate(percent_of_train_data) if j<ideal_ratio] #get all classes that are under-represented\n",
    "        for index in index_under_rep: # each class\n",
    "            print(index)\n",
    "            #determine how many samples needed of this instance\n",
    "            instances_of_class=[iter for iter, el in enumerate(y) if el == index] #get all the indices \n",
    "                                                                            #of the instance in the training set\n",
    "            num_samples_to_add=int(np.ceil((ideal_ratio-act_ratio[index])*len(X))) #find necessary amount of additional class examples needed\n",
    "            print(num_samples_to_add)\n",
    "            for i in range(num_samples_to_add):\n",
    "                #take a random sample from the class and noisify, blur, darken or lighten \n",
    "                alter=random.randint(0,4) #random number from 0-4\n",
    "                \n",
    "                random_index=instances_of_class[random.randint(0, len(instances_of_class)-1)]\n",
    "                image_to_alter= X[random_index]\n",
    "                #print(\"Random Selection\")\n",
    "                #print(image_to_alter.shape)\n",
    "                if alter == 1:\n",
    "                    altered_image=Noisify_Image(image_to_alter)\n",
    "                elif alter==2:\n",
    "                    altered_image=Darken_Image(image_to_alter) #Translate_Image(image_to_alter)\n",
    "                elif alter==3:\n",
    "                    #angle=np.random.randn()*360\n",
    "                    #print(image_to_alter)\n",
    "                    #print(angle)\n",
    "                    altered_image=Lighten_Image(image_to_alter) #Rotate_Image(image_to_alter,angle)\n",
    "                elif alter==4:\n",
    "                    altered_image=Blur_Image(image_to_alter)\n",
    "                else:\n",
    "                    altered_image=X[random_index]\n",
    "                    y_train=np.append(y,index)\n",
    "                    \n",
    "                X,y=Add_Image_to_Array(X, y, altered_image,index) #add distorted image to existing training data\n",
    "    return X, y\n",
    "    \n",
    "            \n",
    "                \n",
    "                \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying distortions for fake image data, ranging from blurring, cropping, rotating, noisifying, etc.\n",
    "import cv2\n",
    "\n",
    "def Noisify_Image(X):\n",
    "    #a,b are start and stop vaues for random number generator\n",
    "    X=X+[np.random.randn()*(np.max(X)-np.min(X))*0.01]\n",
    "    #print(\"Noisify\")\n",
    "    return X\n",
    "def Darken_Image(X):\n",
    "    X=X-np.random.rand()*(np.max(X)/5)\n",
    "    return X\n",
    "def Lighten_Image(X):\n",
    "    X=X-np.random.rand()*(np.max(X)/5)\n",
    "    return X\n",
    "    \n",
    "def Rotate_Image(image,angle):\n",
    "    num_rows, num_cols=image.shape[:2]\n",
    "    image_center=(num_cols/2, num_rows/2)\n",
    "    rot_mat=cv2.getRotationMatrix2D(image_center, angle,1)\n",
    "    result=cv2.warpAffine(image, rot_mat, (image.shape[1], image.shape[0]), flags=cv2.INTER_LINEAR)\n",
    "    #print(\"Rotate\")\n",
    "    return result\n",
    "def Translate_Image(image):\n",
    "    num_rows, num_cols=image.shape[:2]\n",
    "    #transltn_mtx=np.float32([1, 0, np.random.rand(1,1)*32], [0,1,np.random.rand(1,1)])#np.array([np.random.randn(1,3), np.random.randn(1,3)])\n",
    "    #translate_image=np.float32(np.random.randn(1,3)[0], np.random.randn(1,3)[0])\n",
    "    x_rand=(np.random.rand()*(num_cols/10) - (num_cols/10))\n",
    "    y_rand=(np.random.rand()*(num_rows/10) - (num_rows/10))\n",
    "    transltn_mtx=np.float32([[1,0, x_rand], [0,1,y_rand]])\n",
    "    image_translated=cv2.warpAffine(image, transltn_mtx, (num_cols, num_rows))\n",
    "    #print(\"Translate\")\n",
    "    return image_translated\n",
    "def Blur_Image(image):\n",
    "    #now blur image a little\n",
    "    kernel = np.ones(random.randint(1,4), np.float32)\n",
    "    image_blurred=cv2.filter2D(image,-1,kernel)\n",
    "    #print(\"Blur\")\n",
    "    return image_blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample data\n",
    "X_train,y_train=Balanced_Resampling(X_train,y_train,n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram shows imbalance in data set, in order to get better accuracy\n",
    "#one approach would be to balance the training data by subsampling with noise, rotation of various degrees, cropping\n",
    "#added to various images until all training classes are approximately balanced, i.e. uniform\n",
    "\n",
    "counts=np.histogram(y_train,len(set(y_train))) #get the counts of each class in the training set\n",
    "percent_of_train_data=(counts[0]/len(X_train)) #get the percentage of each class represented in the training set\n",
    "\n",
    "ideal_ratio=1/n_classes #get the ideal percentage assuming uniform distribution\n",
    "\n",
    "\n",
    "min_count=sorted(percent_of_train_data)[0] #find the least number of instances of a class\n",
    "\n",
    "max_count=sorted(percent_of_train_data, reverse=True)[0] #find the greatest number of instances of a class\n",
    "\n",
    "index_under_rep=[iter for iter,j in enumerate(percent_of_train_data) if j<ideal_ratio] #find all classes that are under-represented\n",
    "index_over_rep=[iter for iter,j in enumerate(percent_of_train_data) if j>ideal_ratio] #find all classes that are over-represented\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data Set (normalization, grayscale, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def Resize_Image_And_Pad(image, dim_size):\n",
    "    image_decoded=tf.image.decode_image(image,channels=3)\n",
    "    image_resized=tf.resize_images(image, [dim_size-4,dim_size-4])\n",
    "    image_padded=np.pad(image_resized, ((0,0),(2,2),(2,2),(0,0)),'constant') \n",
    "    return image_padded\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Preprocess the data here. \n",
    "def preprocess_example(example):\n",
    "    example=np.divide(np.subtract(example,128, dtype=float),128)\n",
    "    return example\n",
    "\n",
    "def Convert_to_Grayscale(example):\n",
    "    example=cv2.cvtColor(example, cv2.COLOR_RGB2GRAY)\n",
    "    return example\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize_DataSet(X_train, X_valid, X_test):\n",
    "    for iter, image in enumerate(X_train):\n",
    "        X_train[iter]=preprocess_example(X_train[iter])\n",
    "    for iter, image in enumerate(X_valid):\n",
    "        X_valid[iter]=preprocess_example(X_valid[iter])\n",
    "    for iter, image in enumerate(X_test):\n",
    "        X_test[iter]=preprocess_example(X_test[iter])\n",
    "    return X_train, X_test, X_valid\n",
    "X_train, X_test, X_valid=Normalize_DataSet(X_train, X_valid, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iter, image in enumerate(X_train):\n",
    "    if iter==0:\n",
    "        X_train_gray=Convert_to_Grayscale(X_train[iter])\n",
    "    else:\n",
    "        X_train_gray=np.vstack(X_train_gray, Convert_to_Grayscale(X_train[iter]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up Input Variables\n",
    "def set_up_variables(dim_size, n_classes):\n",
    "    x=tf.placeholder(tf.float32, (None, dim_size, dim_size,3))\n",
    "    y=tf.placeholder(tf.int32, (None))\n",
    "    one_hot_y=tf.one_hot(y,n_classes)\n",
    "    return x,y, one_hot_y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up model architecture\n",
    "\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def calculate_dim(W,H, D, F, P, S):\n",
    "    print(W)\n",
    "    print(H)\n",
    "    print(D)\n",
    "    print(F)\n",
    "    print(P)\n",
    "    print(S)\n",
    "    W_out=((W-F+2*P)/S)+1\n",
    "    H_out=((H-F+2*P)/S) +1\n",
    "    D_out=D\n",
    "    volume=H_out*D_out*W_out\n",
    "    return W_out, H_out, D_out, volume\n",
    "\n",
    "\n",
    "def compute_kern_size(W,H,D):\n",
    "    return np.ceil(W*0.15)\n",
    "\n",
    "\n",
    "\n",
    "def ensure_min_kernel(kern_size):\n",
    "    if kern_size <2:\n",
    "        kern_size=2 #make sure min_kern size is 2!\n",
    "    return kern_size\n",
    "\n",
    "def convolve_output_and_activate(input, Weights, bias,keep_prob, num_strides, padding_style):\n",
    "    output=tf.nn.conv2d(input, Weights, strides=num_strides, padding=padding_style)+bias\n",
    "    output=tf.nn.relu(output)\n",
    "    output=tf.nn.dropout(output, keep_prob)\n",
    "    return output\n",
    "\n",
    "def compute_activation(input, Weights, bias,keep_prob):\n",
    "    output=tf.matmul(input,Weights)+bias\n",
    "    activation_output=tf.nn.relu(output)\n",
    "    activation_output=tf.nn.dropout(activation_output, keep_prob)\n",
    "    return activation_output\n",
    "    \n",
    "#compute_kern_output_depth=lambda(W,H,D, kern_size): return W*H*D*kern_size\n",
    "\n",
    "#First Convolutional layer:\n",
    "def LeNet(x, keep_prob):\n",
    "    #Distribution parameters for weight initialization\n",
    "    mu=0\n",
    "    sigma=0.1\n",
    "    \n",
    "    #define 1st layer filter\n",
    "    x_dim=x.get_shape().as_list()[1:]\n",
    "    \n",
    "    kern_size=13#9 #int(x_dim[0]) #3\n",
    "    kern_size_list=[1,kern_size, kern_size, x_dim[2]]\n",
    "    \n",
    "    num_chan=x_dim[2] #x_dim[2]#3; depth of image (i.e. RGB or grayscale?)\n",
    "    num_filt=10 #number of filters to use to convolve over image\n",
    "    pad_type='VALID'\n",
    "    \n",
    "    strides_list=[1,1,1,1]\n",
    "    conv_W1=tf.Variable(tf.truncated_normal(shape=(kern_size,kern_size,num_chan,num_filt), mean=mu, stddev=sigma))\n",
    "    \n",
    "    conv_b1=tf.Variable(tf.zeros(num_filt))\n",
    "    conv1=convolve_output_and_activate(x, conv_W1, conv_b1, keep_prob, strides_list, pad_type)\n",
    "    \n",
    "    #Pooling\n",
    "        \n",
    "    conv1=tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,1,1,1], padding=pad_type)\n",
    "    x_dim=conv1.get_shape().as_list()[1:]\n",
    "    \n",
    "    #2nd convolutional layer\n",
    "    \n",
    "    kern_size=7\n",
    "    kern_depth=int(x_dim[2])\n",
    "    kern_size_list=[1, kern_size, kern_size, x_dim[2]]\n",
    "    num_filt=24\n",
    "    \n",
    "    conv_W2=tf.Variable(tf.truncated_normal(shape=(kern_size,kern_size,kern_depth,num_filt), mean=mu, stddev=sigma))\n",
    "    conv_b2=tf.Variable(tf.zeros(num_filt))\n",
    "    \n",
    "    conv2=convolve_output_and_activate(conv1, conv_W2, conv_b2, keep_prob,strides_list, pad_type)\n",
    "    conv2=tf.nn.max_pool(conv2, ksize=[1,4,4,1], strides=[1,2,2,1], padding=pad_type)\n",
    "    \n",
    "    \n",
    "    #flatten into fully connected layer\n",
    "    \n",
    "    flattened_layer0=flatten(conv2) #layer is now size of vol\n",
    "    x_dim=flattened_layer0.get_shape().as_list()[1:]\n",
    "    \n",
    "    num_output_neurons=120\n",
    "    flat_layer_W0=tf.Variable(tf.truncated_normal(shape=(int(x_dim[0]), num_output_neurons), mean=mu, stddev=sigma))\n",
    "    flat_layer_b0=tf.Variable(tf.zeros(num_output_neurons))\n",
    "    \n",
    "    flat_layer_0_output=compute_activation(flattened_layer0, flat_layer_W0, flat_layer_b0, keep_prob)\n",
    "    \n",
    "    \n",
    "    #fully connected layer 1\n",
    "    x_dim=flat_layer_0_output.get_shape().as_list()[1:]\n",
    "    \n",
    "    num_output_neurons=84\n",
    "    flat_layer_W1=tf.Variable(tf.truncated_normal(shape=(int(x_dim[0]), num_output_neurons), mean=mu, stddev=sigma))\n",
    "    flat_layer_b1=tf.Variable(tf.zeros(num_output_neurons))\n",
    "    \n",
    "    \n",
    "    flat_layer_1_output=compute_activation(flat_layer_0_output, flat_layer_W1, flat_layer_b1, keep_prob)\n",
    "    \n",
    "    #fully connected layer 2\n",
    "    x_dim=flat_layer_1_output.get_shape().as_list()[1:]\n",
    "    \n",
    "    flat_layer_W2=tf.Variable(tf.truncated_normal(shape=(x_dim[0], n_classes), mean=mu, stddev=sigma))\n",
    "    flat_layer_b2=tf.Variable(tf.zeros(n_classes))\n",
    "    \n",
    "    \n",
    "    logits=compute_activation(flat_layer_1_output, flat_layer_W2, flat_layer_b2, keep_prob)#tf.nn.relu(final_output)\n",
    "    print(logits.get_shape().as_list())\n",
    "    return logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dim_size=32\n",
    "num_training_examples=len(X_train)\n",
    "x,y,one_hot_y=set_up_variables(dim_size, n_classes)\n",
    "learnrate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 43]\n",
      "Tensor(\"one_hot:0\", dtype=float32)\n",
      "(?, 43)\n"
     ]
    }
   ],
   "source": [
    "keep_prob=tf.placeholder(tf.float32)\n",
    "logits=LeNet(x,keep_prob) #get the model estimation\n",
    "print(one_hot_y)\n",
    "print(logits.get_shape())\n",
    "cross_entropy=tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y,logits=logits) #get difference between ground truth and the logits\n",
    "loss_function=tf.reduce_mean(cross_entropy)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learnrate)\n",
    "training_operation=optimizer.minimize(loss_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_pred=tf.equal(tf.argmax(logits,1), tf.argmax(one_hot_y,1))\n",
    "incorrect_pred=tf.not_equal(tf.argmax(logits,1), tf.argmax(one_hot_y,1))\n",
    "accuracy_pred=tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "saver=tf.train.Saver()\n",
    "def evaluate_validation(X_data, Y_data):\n",
    "    num_examples=len(X_data)\n",
    "    total_accuracy=0\n",
    "    tf.sess=tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x,batch_y=X_data[offset:offset+BATCH_SIZE], Y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy=sess.run(accuracy_pred, feed_dict={x:batch_x, y:batch_y, keep_prob:1.0})\n",
    "        total_accuracy+=(accuracy*len(batch_x))\n",
    "    return total_accuracy/num_examples\n",
    "def evaluate(X_data, Y_data):\n",
    "    num_examples=len(X_data)\n",
    "    total_accuracy=0\n",
    "    tf.sess=tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x,batch_y=X_data[offset:offset+BATCH_SIZE], Y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy=sess.run(accuracy_pred, feed_dict={x:batch_x, y:batch_y})\n",
    "        total_accuracy+=(accuracy*len(batch_x))\n",
    "    return total_accuracy/num_examples\n",
    "def evaluate_train_accuracy(X_data, Y_data):\n",
    "    num_examples=len(X_data)\n",
    "    total_accuracy=0\n",
    "    tf.sess=tf.get_default_session()\n",
    "    accuracy=sess.run(accuracy_pred, feed_dict={x:X_data, y:Y_data})\n",
    "\n",
    "    #    total_accuracy+=(accuracy*len(batch_x))\n",
    "    return accuracy\n",
    "def evaluate_precision(X_data, Y_data):\n",
    "    num_examples=len(X_data) #number of predictions made\n",
    "    accuracy=sess.run(accuracy_pred, feed_dict={x:X_data, y:Y_data})\n",
    "    precision=accuracy/num_examples\n",
    "    return precision\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1.... \n",
      "\n",
      "Validation accuracy=0.304308\n",
      "EPOCH 2.... \n",
      "\n",
      "Validation accuracy=0.503628\n",
      "EPOCH 3.... \n",
      "\n",
      "Validation accuracy=0.611791\n",
      "EPOCH 4.... \n",
      "\n",
      "Validation accuracy=0.697506\n",
      "EPOCH 5.... \n",
      "\n",
      "Validation accuracy=0.739683\n",
      "EPOCH 6.... \n",
      "\n",
      "Validation accuracy=0.799773\n",
      "EPOCH 7.... \n",
      "\n",
      "Validation accuracy=0.822222\n",
      "EPOCH 8.... \n",
      "\n",
      "Validation accuracy=0.856236\n",
      "EPOCH 9.... \n",
      "\n",
      "Validation accuracy=0.864853\n",
      "EPOCH 10.... \n",
      "\n",
      "Validation accuracy=0.872336\n",
      "EPOCH 11.... \n",
      "\n",
      "Validation accuracy=0.888889\n",
      "EPOCH 12.... \n",
      "\n",
      "Validation accuracy=0.898186\n",
      "EPOCH 13.... \n",
      "\n",
      "Validation accuracy=0.899320\n",
      "EPOCH 14.... \n",
      "\n",
      "Validation accuracy=0.902948\n",
      "EPOCH 15.... \n",
      "\n",
      "Validation accuracy=0.906576\n",
      "EPOCH 16.... \n",
      "\n",
      "Validation accuracy=0.905442\n",
      "EPOCH 17.... \n",
      "\n",
      "Validation accuracy=0.910658\n",
      "EPOCH 18.... \n",
      "\n",
      "Validation accuracy=0.910658\n",
      "EPOCH 19.... \n",
      "\n",
      "Validation accuracy=0.914512\n",
      "EPOCH 20.... \n",
      "\n",
      "Validation accuracy=0.916327\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "#Actually training the model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) #initialize the variables\n",
    "    num_examples=len(X_train)\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        X_train,y_train=shuffle(X_train,y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            batch_x,batch_y=X_train[offset:offset+BATCH_SIZE], y_train[offset:offset+BATCH_SIZE]\n",
    "            \n",
    "            sess.run(training_operation, feed_dict={x:batch_x, y:batch_y,keep_prob:0.8}) #prev 0.8 kp gave 90% validation accuracy\n",
    "            \n",
    "        print(\"EPOCH {}.... \".format(epoch+1))\n",
    "        \n",
    "        print()\n",
    "        validation_accuracy=evaluate_validation(X_valid, y_valid)\n",
    "        print(\"Validation accuracy={:3f}\".format(validation_accuracy))        \n",
    "    try:\n",
    "        saver\n",
    "    except NameError:\n",
    "        saver=tf.train.Saver()\n",
    "    saver.save(sess,\"./Convnet\")\n",
    "    print(\"Saved Model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Convnet\n",
      "Test Accuracy = 0.909343\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'./Convnet')\n",
    "    test_accuracy=evaluate_validation(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:3f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
